{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ¾ Rice Classification | Part 2: Model Implementation\n",
    "**Arborio vs Jasmine â€” Binary Classification**\n",
    "\n",
    "**Prerequisite:** Run Part 1 first and ensure `part1_data.pkl` is in your Colab session.\n",
    "\n",
    "**Models built here:** Logistic Regression Â· Random Forest Â· SVM Â· KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 1: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle, time, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model   import LogisticRegression\n",
    "from sklearn.ensemble       import RandomForestClassifier\n",
    "from sklearn.svm            import SVC\n",
    "from sklearn.neighbors      import KNeighborsClassifier\n",
    "from sklearn.metrics        import accuracy_score, classification_report\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='Set2')\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print('âœ… Imports ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 2: Load Preprocessed Data from Part 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open('part1_data.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "\n",
    "X_train, X_val, X_test = d['X_train'], d['X_val'], d['X_test']\n",
    "y_train, y_val, y_test = d['y_train'], d['y_val'], d['y_test']\n",
    "feature_columns        = d['feature_columns']\n",
    "\n",
    "print(f'Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 3: Evaluation Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = {}  # Stores all model results for the final comparison\n",
    "\n",
    "def report(name, model, X_v, y_v, t):\n",
    "    \"\"\"Evaluate model on validation set and store results.\"\"\"\n",
    "    preds  = model.predict(X_v)\n",
    "    acc    = accuracy_score(y_v, preds)\n",
    "    results[name] = {'model': model, 'val_acc': acc, 'preds': preds, 'time': t}\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'  {name}')\n",
    "    print(f'  Val Accuracy : {acc*100:.2f}%   |   Train time: {t:.3f}s')\n",
    "    print(classification_report(y_v, preds, target_names=['Arborio','Jasmine']))\n",
    "\n",
    "print('âœ… Helper ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 â€” Baseline: Logistic Regression\n",
    "Draws a straight boundary between the two classes. Fast and interpretable â€” our performance floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 4: Tune C (regularisation strength inverse) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Small C = stronger regularisation (simpler model)\n",
    "# Large C = weaker regularisation (more complex model, risk of overfitting)\n",
    "\n",
    "print(f'{\"C\":<10} {\"Val Acc\":>10} {\"Train Acc\":>11}')\n",
    "print('-' * 34)\n",
    "\n",
    "lr_tuning = []\n",
    "for C in [0.01, 0.1, 1.0, 10.0, 100.0]:\n",
    "    m = LogisticRegression(C=C, max_iter=1000, random_state=RANDOM_SEED)\n",
    "    m.fit(X_train, y_train)\n",
    "    tr = accuracy_score(y_train, m.predict(X_train))\n",
    "    va = accuracy_score(y_val,   m.predict(X_val))\n",
    "    lr_tuning.append({'C': C, 'val': va, 'train': tr, 'model': m})\n",
    "    print(f'C={C:<7} Val: {va*100:>6.2f}%   Train: {tr*100:>6.2f}%')\n",
    "\n",
    "best_lr  = max(lr_tuning, key=lambda r: r['val'])\n",
    "BEST_C   = best_lr['C']\n",
    "print(f'\\nğŸ† Best C: {BEST_C}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 5: Plot LR Tuning Curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "c_labels  = [str(r['C'])    for r in lr_tuning]\n",
    "train_acc = [r['train']*100 for r in lr_tuning]\n",
    "val_acc   = [r['val']*100   for r in lr_tuning]\n",
    "\n",
    "ax.plot(c_labels, train_acc, 'o-', label='Train', color='#4C72B0', linewidth=2)\n",
    "ax.plot(c_labels, val_acc,   's-', label='Val',   color='#DD8452', linewidth=2)\n",
    "ax.fill_between(c_labels, train_acc, val_acc, alpha=0.1, color='red', label='Gap')\n",
    "ax.set_xlabel('C Value'); ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Logistic Regression â€” Tuning Curve', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 6: Train Final LR Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t0 = time.time()\n",
    "final_lr = LogisticRegression(C=BEST_C, max_iter=1000, random_state=RANDOM_SEED)\n",
    "final_lr.fit(X_train, y_train)\n",
    "report('Logistic Regression (Baseline)', final_lr, X_val, y_val, time.time()-t0)\n",
    "\n",
    "# Feature coefficients â€” positive pushes toward Jasmine, negative toward Arborio\n",
    "coefs = pd.Series(final_lr.coef_[0], index=feature_columns).sort_values(key=abs, ascending=False)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.barh(coefs.index, coefs.values,\n",
    "        color=['#DD8452' if v>0 else '#4C72B0' for v in coefs.values])\n",
    "ax.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax.set_title('LR Feature Coefficients | Orangeâ†’Jasmine  Blueâ†’Arborio', fontweight='bold')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2a â€” Advanced Model 1: Random Forest\n",
    "Ensemble of many Decision Trees. Handles non-linear patterns. Gives feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 7: Tune Random Forest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# n_estimators : number of trees (more = stabler, slower)\n",
    "# max_depth    : how deep each tree can grow (None = unlimited)\n",
    "# max_features : features considered at each split ('sqrt' is classic default)\n",
    "\n",
    "rf_configs = [\n",
    "    {'n_estimators': 50,  'max_depth': 5,    'max_features': 'sqrt'},\n",
    "    {'n_estimators': 100, 'max_depth': 10,   'max_features': 'sqrt'},\n",
    "    {'n_estimators': 200, 'max_depth': None, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 100, 'max_depth': 10,   'max_features': 'log2'},\n",
    "    {'n_estimators': 200, 'max_depth': None, 'max_features': 'log2'},\n",
    "]\n",
    "\n",
    "rf_tuning  = []\n",
    "print(f'{\"#\":<4} {\"n_est\":>6} {\"depth\":>7} {\"feats\":>7} {\"Val\":>8} {\"Train\":>9}')\n",
    "print('-' * 47)\n",
    "\n",
    "for i, cfg in enumerate(rf_configs):\n",
    "    m = RandomForestClassifier(**cfg, min_samples_split=5,\n",
    "                               random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    m.fit(X_train, y_train)\n",
    "    tr = accuracy_score(y_train, m.predict(X_train))\n",
    "    va = accuracy_score(y_val,   m.predict(X_val))\n",
    "    depth = str(cfg['max_depth']) if cfg['max_depth'] else 'None'\n",
    "    rf_tuning.append({**cfg, 'val': va, 'train': tr, 'model': m})\n",
    "    print(f'{i+1:<4} {cfg[\"n_estimators\"]:>6} {depth:>7} {cfg[\"max_features\"]:>7} {va*100:>7.2f}% {tr*100:>8.2f}%')\n",
    "\n",
    "best_rf = max(rf_tuning, key=lambda r: r['val'])\n",
    "print(f'\\nğŸ† Best: n_estimators={best_rf[\"n_estimators\"]}, depth={best_rf[\"max_depth\"]}, feats={best_rf[\"max_features\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 8: Train Final RF + Feature Importance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t0 = time.time()\n",
    "final_rf = RandomForestClassifier(\n",
    "    n_estimators=best_rf['n_estimators'], max_depth=best_rf['max_depth'],\n",
    "    max_features=best_rf['max_features'], min_samples_split=5,\n",
    "    random_state=RANDOM_SEED, n_jobs=-1)\n",
    "final_rf.fit(X_train, y_train)\n",
    "report('Random Forest', final_rf, X_val, y_val, time.time()-t0)\n",
    "\n",
    "importances = pd.Series(final_rf.feature_importances_, index=feature_columns).sort_values()\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.barh(importances.index, importances.values,\n",
    "        color=plt.cm.YlOrRd(importances.values / importances.max()))\n",
    "ax.set_xlabel('Feature Importance (mean impurity reduction)')\n",
    "ax.set_title('Random Forest â€” Feature Importance', fontweight='bold')\n",
    "for bar, val in zip(ax.patches, importances.values):\n",
    "    ax.text(val+0.001, bar.get_y()+bar.get_height()/2, f'{val*100:.1f}%', va='center', fontsize=9)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2b â€” Advanced Model 2: Support Vector Machine (SVM)\n",
    "Finds the widest margin boundary between classes. RBF kernel handles non-linear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 9: Tune SVM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# kernel : 'linear' = straight boundary | 'rbf' = curved boundary\n",
    "# C      : regularisation (same concept as LR)\n",
    "# gamma  : how far each training point's influence reaches ('scale' = safe auto-setting)\n",
    "\n",
    "svm_configs = [\n",
    "    {'kernel': 'linear', 'C': 0.1,  'gamma': 'scale'},\n",
    "    {'kernel': 'linear', 'C': 1.0,  'gamma': 'scale'},\n",
    "    {'kernel': 'rbf',    'C': 1.0,  'gamma': 'scale'},\n",
    "    {'kernel': 'rbf',    'C': 10.0, 'gamma': 'scale'},\n",
    "    {'kernel': 'rbf',    'C': 10.0, 'gamma': 'auto'},\n",
    "]\n",
    "\n",
    "svm_tuning = []\n",
    "print(f'{\"#\":<4} {\"kernel\":>8} {\"C\":>6} {\"gamma\":>7} {\"Val\":>8} {\"Train\":>9}')\n",
    "print('-' * 48)\n",
    "\n",
    "for i, cfg in enumerate(svm_configs):\n",
    "    m = SVC(**cfg, probability=True, random_state=RANDOM_SEED)\n",
    "    m.fit(X_train, y_train)\n",
    "    tr = accuracy_score(y_train, m.predict(X_train))\n",
    "    va = accuracy_score(y_val,   m.predict(X_val))\n",
    "    svm_tuning.append({**cfg, 'val': va, 'train': tr, 'model': m})\n",
    "    print(f'{i+1:<4} {cfg[\"kernel\"]:>8} {cfg[\"C\"]:>6} {cfg[\"gamma\"]:>7} {va*100:>7.2f}% {tr*100:>8.2f}%')\n",
    "\n",
    "best_svm = max(svm_tuning, key=lambda r: r['val'])\n",
    "print(f'\\nğŸ† Best: kernel={best_svm[\"kernel\"]}, C={best_svm[\"C\"]}, gamma={best_svm[\"gamma\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 10: Train Final SVM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t0 = time.time()\n",
    "final_svm = SVC(kernel=best_svm['kernel'], C=best_svm['C'],\n",
    "                gamma=best_svm['gamma'], probability=True, random_state=RANDOM_SEED)\n",
    "final_svm.fit(X_train, y_train)\n",
    "report('Support Vector Machine', final_svm, X_val, y_val, time.time()-t0)\n",
    "\n",
    "sv = final_svm.n_support_\n",
    "print(f'\\nSupport vectors â€” Arborio: {sv[0]} | Jasmine: {sv[1]} | Total: {sum(sv)}')\n",
    "print('(Fewer support vectors = wider, more confident margin)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2c â€” Advanced Model 3: K-Nearest Neighbors (KNN)\n",
    "No training phase â€” classifies by majority vote from K most similar training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 11: Tune KNN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# n_neighbors (K) : how many neighbors vote  â€” small K=overfit, large K=underfit\n",
    "# weights         : 'uniform'=equal vote | 'distance'=closer neighbors vote more\n",
    "\n",
    "knn_tuning = []\n",
    "print(f'{\"K\":>4} {\"weights\":>10} {\"Val\":>8} {\"Train\":>9}')\n",
    "print('-' * 35)\n",
    "\n",
    "for k in [1, 3, 5, 7, 11, 15, 21, 31, 51]:\n",
    "    for w in ['uniform', 'distance']:\n",
    "        m = KNeighborsClassifier(n_neighbors=k, weights=w, metric='euclidean')\n",
    "        m.fit(X_train, y_train)\n",
    "        tr = accuracy_score(y_train, m.predict(X_train))\n",
    "        va = accuracy_score(y_val,   m.predict(X_val))\n",
    "        knn_tuning.append({'k': k, 'w': w, 'val': va, 'train': tr, 'model': m})\n",
    "        print(f'K={k:<3} {w:>10} {va*100:>7.2f}% {tr*100:>8.2f}%')\n",
    "\n",
    "best_knn = max(knn_tuning, key=lambda r: r['val'])\n",
    "print(f'\\nğŸ† Best: K={best_knn[\"k\"]}, weights={best_knn[\"w\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 12: KNN Tuning Curve (Bias-Variance Tradeoff) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for weight_type in ['uniform', 'distance']:\n",
    "    subset = [r for r in knn_tuning if r['w'] == weight_type]\n",
    "    k_vals = [r['k']        for r in subset]\n",
    "    tr_acc = [r['train']*100 for r in subset]\n",
    "    va_acc = [r['val']*100   for r in subset]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    ax.plot(k_vals, tr_acc, 'o-', label='Train', color='#4C72B0', linewidth=2)\n",
    "    ax.plot(k_vals, va_acc, 's-', label='Val',   color='#DD8452', linewidth=2)\n",
    "    ax.fill_between(k_vals, tr_acc, va_acc, alpha=0.1, color='red')\n",
    "    best_k = max(subset, key=lambda r: r['val'])['k']\n",
    "    ax.axvline(best_k, color='green', linestyle='--', linewidth=1.5, label=f'Best K={best_k}')\n",
    "    ax.set_xlabel('K'); ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'KNN Tuning â€” {weight_type} weights | Small K=overfit, Large K=underfit', fontweight='bold')\n",
    "    ax.legend(); ax.set_xticks(k_vals)\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 13: Train Final KNN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t0 = time.time()\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_knn['k'],\n",
    "                                  weights=best_knn['w'], metric='euclidean')\n",
    "final_knn.fit(X_train, y_train)\n",
    "report('K-Nearest Neighbors', final_knn, X_val, y_val, time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 14: Model Comparison Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "comp = pd.DataFrame([\n",
    "    {'Model': name, 'Val Accuracy (%)': round(r['val_acc']*100, 2), 'Time (s)': round(r['time'], 3)}\n",
    "    for name, r in results.items()\n",
    "]).sort_values('Val Accuracy (%)', ascending=False).reset_index(drop=True)\n",
    "comp.index += 1\n",
    "\n",
    "print('\\nğŸ“Š MODEL COMPARISON (Validation Set)')\n",
    "print('=' * 55)\n",
    "print(comp.to_string())\n",
    "print('=' * 55)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "names  = comp['Model'].tolist()\n",
    "accs   = comp['Val Accuracy (%)'].tolist()\n",
    "times  = comp['Time (s)'].tolist()\n",
    "colors = ['#2ecc71' if i==0 else '#3498db' for i in range(len(names))]\n",
    "\n",
    "bars = axes[0].barh(names[::-1], accs[::-1], color=colors[::-1], edgecolor='white')\n",
    "axes[0].set_title('Validation Accuracy', fontweight='bold')\n",
    "axes[0].set_xlim(min(accs)-2, 101)\n",
    "for bar, v in zip(bars, accs[::-1]):\n",
    "    axes[0].text(v+0.1, bar.get_y()+bar.get_height()/2, f'{v:.2f}%', va='center', fontweight='bold')\n",
    "\n",
    "axes[1].barh(names[::-1], times[::-1], color='#9b59b6', edgecolor='white')\n",
    "axes[1].set_title('Training Time (s)', fontweight='bold')\n",
    "plt.suptitle('Part 2 Summary: All Models | ğŸŸ¢ = Best accuracy', fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 15: Save All Models for Part 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pickle\n",
    "\n",
    "out = {\n",
    "    'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
    "    'y_train': y_train,  'y_val': y_val,  'y_test': y_test,\n",
    "    'feature_columns': feature_columns,\n",
    "    'models': {\n",
    "        'Logistic Regression':     final_lr,\n",
    "        'Random Forest':           final_rf,\n",
    "        'Support Vector Machine':  final_svm,\n",
    "        'K-Nearest Neighbors':     final_knn,\n",
    "    },\n",
    "    'val_results': results\n",
    "}\n",
    "\n",
    "with open('part2_models.pkl', 'wb') as f:\n",
    "    pickle.dump(out, f)\n",
    "\n",
    "print('ğŸ’¾ Saved to part2_models.pkl')\n",
    "print('   â†’ Open Part 3 notebook next')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
