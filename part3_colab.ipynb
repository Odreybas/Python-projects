{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ¾ Rice Classification | Part 3: Evaluation & Metrics\n",
    "**Arborio vs Jasmine â€” Binary Classification**\n",
    "\n",
    "**Prerequisite:** Run Parts 1 & 2 first. Ensure `part2_models.pkl` is in your Colab session.\n",
    "\n",
    "**Covers:** Accuracy Â· Precision Â· Recall Â· F1 Â· Confusion Matrix Â· ROC AUC Â· Misclassification Analysis Â· Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 1: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_curve, roc_auc_score\n",
    ")\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='Set2')\n",
    "CLASS_NAMES = ['Arborio', 'Jasmine']\n",
    "print('âœ… Imports ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 2: Load Models & Data from Part 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open('part2_models.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "\n",
    "X_train, X_val, X_test = d['X_train'], d['X_val'], d['X_test']\n",
    "y_train, y_val, y_test = d['y_train'], d['y_val'], d['y_test']\n",
    "models                 = d['models']\n",
    "feature_columns        = d['feature_columns']\n",
    "\n",
    "print('Models loaded:', list(models.keys()))\n",
    "print(f'Val size: {len(y_val)} | Test size: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 3: Compute All Metrics for Every Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def full_metrics(model, X, y_true):\n",
    "    \"\"\"Returns dict of all classification metrics for a trained model.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    # predict_proba gives probability scores â€” column 1 = P(Jasmine)\n",
    "    y_prob = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    return {\n",
    "        'y_pred':    y_pred,\n",
    "        'y_prob':    y_prob,\n",
    "        'accuracy':  accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall':    recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1':        f1_score(y_true, y_pred, zero_division=0),\n",
    "        'cm':        confusion_matrix(y_true, y_pred),\n",
    "        'roc_auc':   roc_auc_score(y_true, y_prob) if y_prob is not None else None,\n",
    "    }\n",
    "\n",
    "val_metrics = {name: full_metrics(m, X_val, y_val) for name, m in models.items()}\n",
    "print('âœ… Metrics computed for all models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 4: Comprehensive Metrics Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rows = []\n",
    "for name, m in val_metrics.items():\n",
    "    rows.append({\n",
    "        'Model':     name,\n",
    "        'Accuracy':  f\"{m['accuracy']*100:.2f}%\",\n",
    "        'Precision': f\"{m['precision']*100:.2f}%\",\n",
    "        'Recall':    f\"{m['recall']*100:.2f}%\",\n",
    "        'F1':        f\"{m['f1']*100:.2f}%\",\n",
    "        'ROC AUC':   f\"{m['roc_auc']:.4f}\" if m['roc_auc'] else 'N/A',\n",
    "    })\n",
    "\n",
    "table = pd.DataFrame(rows).set_index('Model')\n",
    "print('ğŸ“Š METRICS TABLE â€” Validation Set')\n",
    "print('=' * 70)\n",
    "print(table.to_string())\n",
    "print('=' * 70)\n",
    "print('\\nMetric guide:')\n",
    "print('  Accuracy  : Overall correct predictions')\n",
    "print('  Precision : Of predicted Jasmine, how many were truly Jasmine')\n",
    "print('  Recall    : Of actual Jasmine grains, how many did we catch')\n",
    "print('  F1        : Harmonic mean of Precision and Recall')\n",
    "print('  ROC AUC   : 0.5=random | 1.0=perfect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 5: Confusion Matrices â€” All Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Diagonal = correct | Off-diagonal = mistakes\n",
    "# Top-right (FP) = Arborio called Jasmine | Bottom-left (FN) = Jasmine called Arborio\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 11))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, m) in enumerate(val_metrics.items()):\n",
    "    cm      = m['cm']\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    annot   = np.array([[f\"{cm[i,j]}\\n({cm_norm[i,j]*100:.1f}%)\" for j in range(2)] for i in range(2)])\n",
    "\n",
    "    sns.heatmap(cm_norm, annot=annot, fmt='', cmap='Blues',\n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                ax=axes[idx], vmin=0, vmax=1, linewidths=2, linecolor='white', cbar=False)\n",
    "\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    axes[idx].set_title(f'{name}\\nAcc: {m[\"accuracy\"]*100:.1f}%  TP={TP} TN={TN} FP={FP} FN={FN}',\n",
    "                        fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual'); axes[idx].set_xlabel('Predicted')\n",
    "\n",
    "plt.suptitle('Confusion Matrices â€” All Models (Validation Set)\\n'\n",
    "             'Diagonal = correct | Off-diagonal = mistakes', fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 6: Radar Chart â€” Multi-Metric Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "metric_keys   = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
    "N      = len(metric_labels)\n",
    "angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist() + \\\n",
    "         np.linspace(0, 2*np.pi, N, endpoint=False).tolist()[:1]  # Close the polygon\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "colors  = ['#4C72B0', '#DD8452', '#55A868', '#C44E52']\n",
    "\n",
    "for (name, m), color in zip(val_metrics.items(), colors):\n",
    "    vals = [m[k] or 0 for k in metric_keys] + [m[metric_keys[0]] or 0]\n",
    "    ax.plot(angles, vals, 'o-', linewidth=2, label=name, color=color)\n",
    "    ax.fill(angles, vals, alpha=0.08, color=color)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metric_labels, fontsize=11, fontweight='bold')\n",
    "ax.set_ylim(0.80, 1.0)\n",
    "ax.set_yticks([0.82, 0.87, 0.92, 0.97])\n",
    "ax.set_yticklabels(['82%','87%','92%','97%'], fontsize=9)\n",
    "ax.set_title('Radar Chart â€” Model Comparison\\nLarger area = stronger overall', fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.35, 1.15), fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('radar_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 7: ROC Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# x-axis: False Positive Rate (false alarms)\n",
    "# y-axis: True Positive Rate (correct detections)\n",
    "# Closer to top-left corner = better | Diagonal = random chance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "colors  = ['#4C72B0', '#DD8452', '#55A868', '#C44E52']\n",
    "\n",
    "for (name, m), color in zip(val_metrics.items(), colors):\n",
    "    if m['y_prob'] is None: continue\n",
    "    fpr, tpr, _ = roc_curve(y_val, m['y_prob'])\n",
    "    ax.plot(fpr, tpr, linewidth=2.5, color=color, label=f'{name}  (AUC={m[\"roc_auc\"]:.4f})')\n",
    "\n",
    "ax.plot([0,1],[0,1], 'k--', linewidth=1.2, label='Random Chance (AUC=0.5)')\n",
    "ax.plot(0, 1, 'g*', markersize=15, label='Perfect Classifier', zorder=5)\n",
    "ax.annotate('Perfect', xy=(0,1), xytext=(0.05, 0.93), color='green', fontsize=9,\n",
    "            arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "ax.set_xlabel('False Positive Rate (FPR) â†’ false alarms', fontsize=11)\n",
    "ax.set_ylabel('True Positive Rate (TPR / Recall)', fontsize=11)\n",
    "ax.set_title('ROC Curves â€” All Models | Higher & further left = better', fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.set_xlim([-0.01, 1.01]); ax.set_ylim([-0.01, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 8: Which Metric Matters Most? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\"\"\n",
    "PRIMARY METRIC: F1 Score\n",
    "\n",
    "In rice classification both error types have similar costs:\n",
    "  FP: Arborio labelled as Jasmine â†’ wrong product delivered\n",
    "  FN: Jasmine labelled as Arborio â†’ premium rice undervalued\n",
    "\n",
    "When neither error is much worse than the other, F1 is the\n",
    "right metric â€” it balances Precision and Recall equally.\n",
    "\n",
    "Precision vs Recall trade-off:\n",
    "  Lower threshold â†’ more Jasmine predictions â†’ higher Recall, lower Precision\n",
    "  Higher threshold â†’ fewer Jasmine predictions â†’ higher Precision, lower Recall\n",
    "  This trade-off is captured in the ROC curve above.\n",
    "\n",
    "ROC AUC is a good secondary metric â€” it is threshold-independent\n",
    "and measures overall ranking ability (0.5=random, 1.0=perfect).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 9: Misclassification Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Find the best model by F1 and examine where it goes wrong\n",
    "\n",
    "best_name    = max(val_metrics, key=lambda k: val_metrics[k]['f1'])\n",
    "best_m       = val_metrics[best_name]\n",
    "y_pred_best  = best_m['y_pred']\n",
    "\n",
    "print(f'Analysing misclassifications for best model: {best_name}')\n",
    "print(f'Val F1: {best_m[\"f1\"]*100:.2f}%\\n')\n",
    "\n",
    "# Build a DataFrame tagging each sample as correct or an error type\n",
    "error_df = pd.DataFrame(X_val, columns=feature_columns)\n",
    "error_df['True']      = ['Arborio' if l==0 else 'Jasmine' for l in y_val]\n",
    "error_df['Predicted'] = ['Arborio' if l==0 else 'Jasmine' for l in y_pred_best]\n",
    "error_df['Correct']   = (y_val == y_pred_best)\n",
    "error_df['Error']     = error_df.apply(\n",
    "    lambda r: 'FP (called Jasmine, was Arborio)' if r['True']=='Arborio' and r['Predicted']=='Jasmine'\n",
    "    else ('FN (called Arborio, was Jasmine)' if r['True']=='Jasmine' and r['Predicted']=='Arborio'\n",
    "    else 'Correct'), axis=1)\n",
    "\n",
    "errors   = error_df[~error_df['Correct']]\n",
    "correct  = error_df[error_df['Correct']]\n",
    "\n",
    "print(f'Correct  : {len(correct)} ({len(correct)/len(y_val)*100:.1f}%)')\n",
    "print(f'Errors   : {len(errors)} ({len(errors)/len(y_val)*100:.1f}%)')\n",
    "print(f'\\nError breakdown:\\n{errors[\"Error\"].value_counts().to_string()}')\n",
    "\n",
    "print('\\nMean feature values â€” Errors vs Correct:')\n",
    "comp = pd.DataFrame({\n",
    "    'Correct':      correct[feature_columns].mean(),\n",
    "    'Misclassified': errors[feature_columns].mean()\n",
    "})\n",
    "comp['Î”%'] = ((comp['Misclassified'] - comp['Correct']) / comp['Correct'] * 100).round(2)\n",
    "print(comp.round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 10: Misclassification Map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Plot errors in the space of the 2 most discriminative features\n",
    "# Errors typically cluster near the decision boundary\n",
    "\n",
    "diff = abs(error_df[error_df['True']=='Arborio'][feature_columns].mean() -\n",
    "           error_df[error_df['True']=='Jasmine'][feature_columns].mean())\n",
    "top2 = diff.nlargest(2).index.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "\n",
    "# Background: all correct samples\n",
    "for label, color in [('Arborio','#6baed6'), ('Jasmine','#fdae6b')]:\n",
    "    mask = correct['True'] == label\n",
    "    ax.scatter(correct.loc[mask, top2[0]], correct.loc[mask, top2[1]],\n",
    "               c=color, alpha=0.3, s=20, label=f'{label} (correct)')\n",
    "\n",
    "# Foreground: errors\n",
    "for err_type, marker, color in [\n",
    "    ('FP (called Jasmine, was Arborio)', 'X', 'red'),\n",
    "    ('FN (called Arborio, was Jasmine)',  'D', 'darkred')\n",
    "]:\n",
    "    mask = errors['Error'] == err_type\n",
    "    if mask.sum() > 0:\n",
    "        ax.scatter(errors.loc[mask, top2[0]], errors.loc[mask, top2[1]],\n",
    "                   c=color, marker=marker, s=120, edgecolors='black',\n",
    "                   zorder=5, label=f'ERROR: {err_type} (n={mask.sum()})')\n",
    "\n",
    "ax.set_xlabel(top2[0], fontsize=11); ax.set_ylabel(top2[1], fontsize=11)\n",
    "ax.set_title(f'Misclassification Map â€” {best_name}\\n'\n",
    "             'Errors cluster near the class boundary â€” the hardest samples to separate',\n",
    "             fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('misclassification_map.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 11: Final Test Set Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Best model selected by val F1 â€” test set evaluated ONCE, here, at the end\n",
    "\n",
    "best_model_obj = models[best_name]\n",
    "test_m         = full_metrics(best_model_obj, X_test, y_test)\n",
    "\n",
    "print(f'ğŸ† Best model: {best_name}')\n",
    "print('=' * 56)\n",
    "print(f'{\"Metric\":<15} {\"Validation\":>12} {\"Test\":>12} {\"Î”\":>10}')\n",
    "print('-' * 56)\n",
    "\n",
    "for key, label in [('accuracy','Accuracy'),('precision','Precision'),\n",
    "                    ('recall','Recall'),('f1','F1'),('roc_auc','ROC AUC')]:\n",
    "    v = best_m[key] or 0\n",
    "    t = test_m[key] or 0\n",
    "    d = t - v\n",
    "    direction = 'â†‘' if d > 0 else ('â†“' if d < 0 else '=')\n",
    "    print(f'{label:<15} {v*100:>11.2f}% {t*100:>11.2f}%  {direction}{abs(d)*100:>5.2f}%')\n",
    "\n",
    "print('=' * 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 12: Val vs Test Confusion Matrices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "for ax, (m, title, n) in zip(axes, [\n",
    "    (best_m,  f'Validation Set ({len(y_val)} samples)',    len(y_val)),\n",
    "    (test_m,  f'Test Set â€” Final Exam ({len(y_test)} samples)', len(y_test))\n",
    "]):\n",
    "    cm      = m['cm']\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    annot   = np.array([[f\"{cm[i,j]}\\n({cm_norm[i,j]*100:.1f}%)\" for j in range(2)] for i in range(2)])\n",
    "    sns.heatmap(cm_norm, annot=annot, fmt='', cmap='Greens',\n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                ax=ax, vmin=0, vmax=1, linewidths=2, linecolor='white', annot_kws={'size':12})\n",
    "    ax.set_title(f'{best_name}\\n{title}\\nAcc: {m[\"accuracy\"]*100:.2f}%  F1: {m[\"f1\"]*100:.2f}%',\n",
    "                 fontweight='bold', fontsize=10)\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual')\n",
    "\n",
    "plt.suptitle('ğŸ† Best Model: Validation vs Test Performance', fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cell 13: Generalisation Verdict â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "gap = abs(best_m['accuracy'] - test_m['accuracy'])\n",
    "\n",
    "if   gap < 0.01: verdict = 'âœ… EXCELLENT â€” virtually no drop on unseen test data'\n",
    "elif gap < 0.03: verdict = 'âœ… GOOD â€” small acceptable drop'\n",
    "elif gap < 0.05: verdict = 'âš ï¸  MODERATE â€” consider more regularisation'\n",
    "else:            verdict = 'âŒ POOR â€” large gap suggests overfitting'\n",
    "\n",
    "print('GENERALISATION ANALYSIS')\n",
    "print('=' * 45)\n",
    "print(f'  Val Accuracy  : {best_m[\"accuracy\"]*100:.2f}%')\n",
    "print(f'  Test Accuracy : {test_m[\"accuracy\"]*100:.2f}%')\n",
    "print(f'  Gap           : {gap*100:.2f}%')\n",
    "print(f'  Verdict       : {verdict}')\n",
    "print('=' * 45)\n",
    "\n",
    "print('\\nâœ… All three parts complete!')\n",
    "print('   Outputs saved: confusion_matrices.png | radar_chart.png | roc_curves.png')\n",
    "print('                  misclassification_map.png | final_evaluation.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
